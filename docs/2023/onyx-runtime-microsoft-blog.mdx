---
title: ONNX Runtime Microsoft Blog
description: Pieces for Developers featured on the Microsoft Open Source Blog, highlighting the partnership between ONNX Runtime (Microsoft) and Pieces.
---

export const EventHeader = props => {
    return <div style={{ display: "flex", flexDirection: "row", flexWrap: "wrap", width: "100%", justifyContent: "start" }}>
        <h1 style={{ marginRight: "16px" }}>{ props.h1 }</h1>
        <div style={{ height: "50%", backgroundColor: "#f3f4f5", borderRadius: "20px", padding: "2px 15px", marginBottom: "16px", marginTop: "2px" }}>
            <p style={{ marginTop: "0px", marginBottom: "0px", fontWeight: "500", color: "black" }}>{ props.p }</p>
        </div>
    </div>
    ;
}

<EventHeader p="February 8, 2023" h1="ONNX Runtime Microsoft Blog"></EventHeader>

Pieces for Developers featured on the Microsoft Open Source Blog, highlighting the partnership between ONNX Runtime (Microsoft) and Pieces.

We embed ML into all of our products to ensure full coverage of the developer workflow. Pieces needs to run these models from the desktop, terminal, integrated development environment, browser, and team communication channels. ONNX Runtime emerged as the clear winner for an on-device model serving platform that would provide Pieces with low latency, privacy, and performance, while still giving machine learning engineers the flexibility that cloud serving offers. ONNX Runtime offers a standardized model optimization pipeline, and Pieces has used ONNX Runtimeâ€™s convert generation.py tool to implement a T5 transformer model for code description generation. ONNX Runtime gracefully meets both performance and state-of-the-art machine learning model needs.

<a href="https://cloudblogs.microsoft.com/opensource/2023/02/08/performant-on-device-inferencing-with-onnx-runtime/">ONYX Runtime Blog Post</a>